{
  "hash": "43dda594ac4651294cb09801385d4dc8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Standard area diagrams\"\neditor_options: \n  chunk_output_type: inline\n---\n\n\n\n\n\n\n\n\n\n\n## Definitions\n\nAccording to a glossary on phytopathometry [@bock2021], standard area diagram (SAD) can be defined as \"*a generic term for a pictorial or graphic representation (drawing or true-color photo) of selected disease severities on plants or plant parts (leaves, fruit, flowers, etc.) generally used as an aid for more accurate visual estimation (on the percentage scale) or classification (using an ordinal scale) of severity on a specimen\".*\n\nThe Standard Area Diagrams (SADs), also known as diagrammatic scales, have a long history of use in plant pathology. The concept dates back to the late 1800s when the Cobb scale was developed, featuring five diagrams depicting a range of severity levels of rust pustules on wheat leaves.\n\nIn the past 20 years, plant pathologists have leveraged advancements in image processing and analysis tools, along with insights from psychophysical and measurement sciences, to develop SADs that are realistic (e.g., true-color photographs), validated, and depict severities that maximize estimation accuracy. SADs have been created in various color formats (black or white, two-color, or true-color) and with varying incremental scales (approximated linear or logarithmic) [@delponte2017].\n\nSADs have proven beneficial in increasing the accuracy of visual estimates, as estimating percentage areas is generally more challenging than classifying severity into ordinal classes - there are numerous possibilities on the percentage scale, compared to the finite and small number of classes in ordinal scales. A recent quantitative review confirmed that using SADs often results in improved accuracy and precision of visual estimates. However, it also identified factors related to SAD design and structure, disease symptoms, and actual severity that affected the outcomes. In particular, SADs have shown greater utility for raters who are inherently less accurate and for diseases characterized by small and numerous lesions [@delponte2022]. Here are examples of SADs in black and white, two-color, and true-color formats:\n\n------------------------------------------------------------------------\n\n![Actual photos of symptoms of loquat scab on fruit (left) and a SADs with eight diagrams (right). Each number represents severity as the percent area affected [@gonzález-domínguez2014]](imgs/sad-loquat.png){#fig-sad-loquat fig-align=\"center\" width=\"625\"}\n\n------------------------------------------------------------------------\n\n![SADs for Glomerella leaf spot on apple leaf. Each number represents severity as the percent area affected [@moreira2018]](imgs/sad-apple.png){#fig-sad-apple fig-align=\"center\" width=\"622\"}\n\n------------------------------------------------------------------------\n\n![SADs for soybean rust. Each number represents severity as the percent area affected [@franceschi2020]](imgs/sad-sbr.png){#fig-sad-sbr fig-align=\"center\" width=\"625\"}\n\nMore SADs can be found in the [SADBank](https://emdelponte.github.io/sadbank/), a curated collection of articles on SAD development and validation. Click on the image below to get access to the database.\n\n[![SADBank, a curated collection of articles](imgs/sadbank.png){#fig-sadbank fig-align=\"center\" width=\"87%\"}](https://emdelponte.github.io/sadbank/)\n\n## SAD development and validation\n\nA systematic review of the literature on SADs highlighted the most important aspects related with the development and validation of the tool [@delponte2017]. A list of best practices was proposed in the review to guide future research in the area. Follows the most important aspects to be noted:\n\n::: callout-tip\n## Best practices on SADs development\n\n-   Sample a minimum number (e.g., n = 100) of specimens from natural epidemics representing the range of disease severity and typical symptoms observed.\n\n-   Use reliable image analysis software to discriminate disease symptoms from healthy areas to calculate percent area affected.\n\n-   When designing the illustrations for the SAD set, ensure that the individual diagrams are prepared realistically, whether line drawn, actual photos, or computer generated.\n\n-   The number of diagrams should be no less than 6 and no more than 10, distributed approximately linearly, and spaced no more than 15% apart. Additional diagrams (±2) should be included between 0 and 10% severity.\n\n-   For the validation trial, select at least 50 specimens representing the full range of actual severity and symptom patterns.\n\n-   When selecting raters (a minimum of 15) for validation, make sure they do not have previous experience in using the SAD under evaluation.\n\n-   Provide standard instructions on how to recognize the symptoms of the disease and how to assess severity, first without and then with the SAD.\n\n-   Ideally repeat the assessment in time, with a 1- or 2-week interval, both without and with the aid, using the same set of raters in order to evaluate the effect of training and experience on gains in accuracy.\n\n-   Both pre- and posttest experiment conditions should be the same to avoid any impact of distraction on accuracy of estimates during the tests.\n:::\n\n## Designing SADs in R\n\nThe diagrams used in a set have been developed using various methods and technologies, ranging from hand-drawn diagrams to actual photographs [@delponte2017]. There is an increasing trend towards using actual photos that are digitally analyzed using standard image analysis software to determine the percent area affected. With this approach, a large set of images is analyzed, and some images are chosen to represent the severities in the SAD according to the scale structure.\n\nIn R, the pliman package has a function called `sad()` which allows the automatic generation of a SADs with a pre-defined number of diagrams. Firstly, as shown in the [previous chapter](data-actual-severity.html), the set of images to be selected needs to be analysed using the `measure_disease()` function. Then, a SADs is automatically generated. In the function, the specimens with the smallest and highest severity will be selected for the SAD. The intermediate diagrams are sampled sequentially to achieve the pre-defined number of images after the severity has been ordered from low to high. More details of the function [here](https://tiagoolivoto.github.io/pliman/reference/sad.html).\n\nLet's use the [same set](data-actual-severity.html#multiple-images) of 10 soybean leaves, as seen in the previous chapter, depicting the rust symptoms and create the `sbr` object.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pliman)\nh <- image_import(\"imgs/sbr_h.png\")\ns <- image_import(\"imgs/sbr_s.png\")\nb <- image_import(\"imgs/sbr_b.png\")\n\nsbr <- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_original = FALSE, # set to TRUE for showing the original.\n  col_background = \"white\", \n  verbose = FALSE,\n  plot = FALSE\n)\n```\n:::\n\n\n\n\n\n\n\n\nWe are ready to run the `sad()` function to create a SADs with five diagrams side by side. The resulting SADs is in two-color as standard. Set the argument `show_original` to `TRUE` for showing the orignal image in the SADs.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsad(sbr, 5, ncol = 5)\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n## Analysis of SADs validation data\n\nTo evaluate the effect of SAD on accuracy components, analyze the data, preferably using concordance analysis methods ([see chapter](data-accuracy.html)), to fully explore which component is affected and to gain insight into the ramification of errors. Linear regression should not be used as the sole method but it could be complementary for comparison with previous literature.\n\nInferential methods should be used for testing hypotheses related to gain in accuracy. If parametric tests are used (paired t-test for example), make sure to check that the assumptions are not violated. Alternatively, nonparametric tests (Wilcoxon signed rank) or nonparametric bootstrapping should be used when the conditions for parametric tests are not met. More recently, a (parametric) mixed modelling framework has been used to analyse SADs validation data where raters are taken as a random effects in the model [@gonzález-domínguez2014; @franceschi2020; @pereira2020].\n\n### Non parametric boostrapping of differences\n\nBootstrap is a resampling method where large numbers of samples of the same size are repeatedly drawn, ***with replacement***, from a single original sample. It is commonly used when the distribution of a statistic is unknown or complicated and the sample size is too small to draw a valid inference.\n\nA bootstrap-based equivalence test procedure was first proposed as complementary to parametric (paired t-test) or non-parametric (Wilcoxon) to analyze severity estimation data in a study on the development and validation of a SADs for pecan scab [@yadav2012]. The equivalence test was used to calculate 95% confidence intervals for each statistic by bootstrapping using the percentile method (with an equivalence test, the null hypothesis is the converse of H0, i.e. the null hypothesis is non-equivalence). In that study, the test was used to compare means of the CCC statistics across raters under two conditions: 1) without versus with the SAD; and 2) experienced versus inexperienced raters.\n\nTo apply the bootstrap-based equivalence test, let's work with the CCC data for a sample of 20 raters who estimated severity of soybean rust SAD first without and then with the aid. The CCC was calculated as shown [here](data-accuracy.html#concordance-correlation-coefficient).\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(r4pde)\n\nsbr <- tibble::tribble(\n  ~rater, ~aided, ~unaided,\n      1,   0.97,     0.85,\n      2,   0.97,     0.85,\n      3,   0.95,     0.82,\n      4,   0.93,     0.69,\n      5,   0.97,     0.84,\n      6,   0.96,     0.86,\n      7,   0.98,     0.78,\n      8,   0.93,     0.72,\n      9,   0.94,     0.67,\n     10,   0.95,     0.53,\n     11,   0.94,     0.78,\n     12,   0.98,     0.89,\n     13,   0.96,      0.8,\n     14,   0.98,     0.87,\n     15,   0.98,      0.9,\n     16,   0.98,     0.87,\n     17,   0.98,     0.84,\n     18,   0.97,     0.86,\n     19,   0.98,     0.89,\n     20,   0.98,     0.78\n  )\n```\n:::\n\n\n\n\n\n\n\n\nLet's visualize the data using boxplots. Each point in the plot represents a rater.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_r4pde())\nsbr |> \n  pivot_longer(2:3, names_to = \"condition\", values_to =\"estimate\") |> \n  ggplot(aes(condition, estimate))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_jitter(width = 0.05, size = 2, alpha = 0.5)+\n  theme_r4pde()+\n  ylim(0.4,1)\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-4-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nTo proceed with bootstrapping, we first create a new variable to hold the differences between the means of the estimates (aided minus unaided). If the 95% CI does not include zero, this means that there was a significant improvement in the statistics.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# diff of means\nsbr$diff <- sbr$aided - sbr$unaided\n\nsbr |> \n  ggplot(aes(x= diff))+\n  theme_r4pde()+\n  geom_histogram(bins = 10, color = \"white\")\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-5-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nUsing the simpleboot and boot packages of R:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(simpleboot)\nb.mean <- one.boot(sbr$diff, mean, 999)\nboot::boot.ci(b.mean)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in boot::boot.ci(b.mean): bootstrap variances needed for studentized\nintervals\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = b.mean)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.1245,  0.1938 )   ( 0.1220,  0.1890 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.130,  0.197 )   ( 0.131,  0.202 )  \nCalculations and Intervals on Original Scale\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(b.mean$data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1595\n```\n\n\n:::\n\n```{.r .cell-code}\nhist(b.mean)\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-6-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nUsing the bootstrap package:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bootstrap)\nb <- bootstrap(sbr$diff, 999, mean)\nquantile(b$thetastar, c(.025,.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    2.5%    97.5% \n0.129500 0.196525 \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(b$thetastar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1594294\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(b$thetastar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01752647\n```\n\n\n:::\n\n```{.r .cell-code}\nse <- function(x) sqrt(var(x)/length(x))\nse(b$thetastar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.000554513\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nBoth procedures shown above have led to similar results. The 95% CIs of the differences did not include zero, so a significant improvement in accuracy can be inferred.\n\n### Parametric and non-parametric paired sample tests\n\nWhen two estimates are gathered from the same rater at different times, these data points are not independent. In such situations, a **paired sample t-test** can be utilized to test if the mean difference between two sets of observations is zero. This test requires each subject (or leaf, in our context) to be measured or estimated twice, resulting in *pairs* of observations. However, if the assumptions of the test (such as normality) are violated, a non-parametric equivalent, such as the Wilcoxon signed-rank test, also known as the **Wilcoxon test**, can be employed. This alternative is particularly useful when the data are not normally distributed.\n\nTo proceed with these tests, we first need to ascertain whether our data are normally distributed. We should also verify whether the variances are equal. Let's now apply these two tests to our data and compare the results.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normality test\nshapiro.test(sbr$aided)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sbr$aided\nW = 0.82529, p-value = 0.002111\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(sbr$unaided)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sbr$unaided\nW = 0.83769, p-value = 0.003338\n```\n\n\n:::\n\n```{.r .cell-code}\n# equal variance test\nvar.test(sbr$aided, sbr$unaided)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tF test to compare two variances\n\ndata:  sbr$aided and sbr$unaided\nF = 0.037789, num df = 19, denom df = 19, p-value = 1.53e-09\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.01495720 0.09547109\nsample estimates:\nratio of variances \n        0.03778862 \n```\n\n\n:::\n\n```{.r .cell-code}\n# paired t-test\nt.test(sbr$aided, sbr$unaided, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  sbr$aided and sbr$unaided\nt = 8.812, df = 19, p-value = 3.873e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1216158 0.1973842\nsample estimates:\nmean difference \n         0.1595 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Wilcoxon test\nwilcox.test(sbr$aided, sbr$unaided, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(sbr$aided, sbr$unaided, paired = TRUE): cannot\ncompute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  sbr$aided and sbr$unaided\nV = 210, p-value = 9.449e-05\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nAs shown above, the two assumptions were violated, so we could rely more confidently on the non-parametric test.\n\n### Mixed effects modeling\n\nMixed models, also known as mixed effects models or multilevel models, are an extension of traditional linear models that are used for analyzing hierarchical or clustered data. These models are particularly useful when dealing with data where observations may not be fully independent, or when the assumption of independence is violated. This happens, for instance, when data are collected over time from the same individuals or units, or when individuals are grouped or nested within higher-level units, such as in our case where measurements are taken by different raters [@brown2021].\n\nMixed models enable us to model both fixed and random effects. Fixed effects represent the usual regression parameters that we are primarily interested in estimating, while random effects model the random variation that occurs at different levels of hierarchy or clustering. They allow us to account for variability among different levels of data, like inter-rater variability or intra-subject variability in repeated measures designs.\n\nIn our context, we consider raters as random effects because we view them as a sample drawn from a larger population of potential raters, and our goal is to generalize our findings to this larger population. If we were to sample additional raters, we would expect these new raters to differ from our current ones. However, by considering raters as a random effect in our model, we can account for this inter-rater variability and make more accurate inferences about the overall population.\n\nThe random effects component in the mixed model allows us to capture and model the additional variance that is not explicitly accounted for by the fixed effects in our model. In other words, random effects help us to capture and quantify the 'unexplained' or 'residual' variation that exists within and between the clusters or groups in our data. This could include, for instance, variation in disease measurements that are taken repeatedly from the same subjects. In conclusion, mixed models provide a robust and flexible framework for modeling hierarchical or clustered data, allowing us to effectively account for both fixed and random effects and to make more accurate inferences about our data.\n\nLet's start reshaping our data to the long format and assign them to a new data frame.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsbr2 <- sbr |> \n  pivot_longer(2:3, names_to = \"condition\", values_to = \"estimate\")\n```\n:::\n\n\n\n\n\n\n\n\nNow we fit the mixed model using the `lmer` function of the *lme4* package. We will fit the model to the logit of the estimate because they should be bounded between zero and one. Preliminary analysis using non-transformed or log-transformed data resulted in lack of normality of residuals and heterocedasticity (not shown).\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4) \nlibrary(car) # for logit function\nmix <- lmer(logit(estimate) ~ condition + (1 | rater), data = sbr2)\n\n# Check model performance\nlibrary(performance)\ncheck_normality(mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOK: residuals appear as normally distributed (p = 0.381).\n```\n\n\n:::\n\n```{.r .cell-code}\ncheck_heteroscedasticity(mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOK: Error variance appears to be homoscedastic (p = 0.961).\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check effect of condition\ncar::Anova(mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: logit(estimate)\n           Chisq Df Pr(>Chisq)    \ncondition 458.44  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimate the means for each group\nlibrary(emmeans)\nem <- emmeans(mix, ~ condition, type =\"response\")\nem\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n condition response      SE   df lower.CL upper.CL\n aided        0.968 0.00359 25.5    0.959    0.974\n unaided      0.817 0.01719 25.5    0.779    0.849\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n```\n\n\n:::\n\n```{.r .cell-code}\n# Contrast the means\npairs(em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast        odds.ratio    SE df null t.ratio p.value\n aided / unaided       6.72 0.597 19    1  21.411  <.0001\n\nDegrees-of-freedom method: kenward-roger \nTests are performed on the log odds ratio scale \n```\n\n\n:::\n\n```{.r .cell-code}\n# plot the means with 95% CIs\nplot(em) +\n  coord_flip()+\n  xlim(0.7,1)+\n  theme_r4pde()\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nAs shown above, we can reject the null hypothesis that the means are the same between the two groups.\n\nAlternatively, we could fit GLMMs - generalized linear mixed models, which extend the traditional linear mixed models to accommodate response variables that follow different distributions. They are particularly useful when the response variable does not follow a normal distribution and cannot be adequately transformed to meet the parametric assumptions of traditional linear models. The glmmTMB package in R provides a convenient and flexible platform to fit GLMMs using a variety of distributions [@brooks2017].\n\nIn our case, considering our response variable bounded between 0 and 1, a Beta distribution might be a suitable choice. Beta distribution is a continuous probability distribution defined on the interval [0, 1], and is commonly used for modelling variables that represent proportions or percentages.\n\nThe function `glmmTMB()` from the *glmmTMB* package can be used to fit a GLMM with a Beta distribution. In this function, we specify the distribution family as `beta_family()`. \n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmmTMB)\nmix2 <-  glmmTMB(estimate ~ condition + (1| rater), \n                 data = sbr2, \n                 family = beta_family())\n```\n:::\n\n\n\n\n\n\n\n\nBecause the package *performance*  does not handle the *glmmTMB* output, we will use the *DHARMa* package in R which can be particularly useful for checking the assumptions of your GLMM fitted with `glmmTMB()`. The package provides a convenient way to carry out residual diagnostics for models fitted via maximum likelihood estimation, including GLMMs. This package creates standardized residuals from the observed responses and the predicted responses of a fitted model, and then compares these residuals to a simulated set of residuals under a correct model. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DHARMa)\n\nplot(simulateResiduals(mix2))\n```\n\n::: {.cell-output-display}\n![](data-sads_files/figure-docx/unnamed-chunk-12-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nIn this example, `simulateResiduals()` generates simulated residuals from your fitted model, and the plot creates a plot of these residuals. This showed that the residuals from our model are uniformly distributed, which is an assumption of GLMMs. We can now proceed with the posthoc analysis and noticed that the results are similar to when the response variable was transformed to logit.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Anova(mix2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: estimate\n           Chisq Df Pr(>Chisq)    \ncondition 400.93  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(emmeans)\nem <- emmeans(mix2, ~ condition, type = \"response\")\nem\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n condition response     SE  df asymp.LCL asymp.UCL\n aided        0.967 0.0043 Inf     0.958     0.975\n unaided      0.814 0.0167 Inf     0.779     0.845\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n```\n\n\n:::\n\n```{.r .cell-code}\n# Contrast the means\npairs(em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast        odds.ratio    SE  df null z.ratio p.value\n aided / unaided       6.71 0.638 Inf    1  20.023  <.0001\n\nTests are performed on the log odds ratio scale \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}